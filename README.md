 Ten-animales-classifier
 =================================

# Introduction: 

This  Python notebook provid a comprehensive image classification project using TensorFlow. It covers data organization, Convolutional Neural Network (CNN) model construction, dataset loading, model training with callbacks, feature extraction from a pre-trained VGG16 model, fine-tuning, and evaluation of the fine-tuned model's performance on a test dataset. This script demonstrates a holistic workflow for image classification.


## Import necessary libraries and modules

- `os`: This module allows interaction with the operating system, enabling file and directory operations.
- `shutil`: The `shutil` module is used for high-level file operations, such as copying and moving files and directories.
- `train_test_split` from `sklearn.model_selection`: This function is essential for splitting datasets into training and testing sets, a common task in machine learning.
- `random`: The `random` module provides functions for generating random numbers and performing randomization, which can be useful for various tasks, including data shuffling.
- `matplotlib.pyplot as plt`: `matplotlib` is a popular library for creating data visualizations. `pyplot` is a module within `matplotlib` that provides a simple and interactive way to create plots and charts.
- `matplotlib.image as mpimg`: The `mpimg` module is used for working with images, including loading and displaying them.
- `tensorflow as tf`: TensorFlow is a deep learning framework, and it is imported as `tf`. It is used for building and training machine learning models.
- `keras` from `tensorflow`: Keras is a high-level neural networks API that runs on top of TensorFlow. It is widely used for defining and training deep learning models.
- `layers, regularizers` from `tensorflow.keras`: These are submodules of the Keras library and provide tools for building and configuring neural network layers and applying regularization techniques to models.
- `numpy as np`: NumPy is a fundamental library for numerical operations in Python. It provides support for arrays, matrices, and various mathematical operations, which are essential for data processing in machine learning.

These imports set the stage for various operations related to data manipulation, model construction, and visualization in the subsequent code.

## Data Directory Setup

- `input_data_dir`: Specifies the directory where the original read-only dataset is located, in this case, "/kaggle/input/animals10/raw-img".
- `output_dir`: Defines the directory where the working dataset will be created, which is "/kaggle/working/dataset".

## Create Subdirectories

- `train_dir`, `validation_dir`, and `test_dir` are defined as subdirectories within the `output_dir`. These will be used to store the training, validation, and test data, respectively.
- `os.makedirs` is used to create these subdirectories if they don't already exist, setting `exist_ok=True`.

## Split Data into Sets

- `class_folders` is a list of class folders (e.g., categories of animals) within the input data directory.
- `train_classes`, `validation_classes`, and `test_classes` are generated by splitting the class folders into training, validation, and testing sets using `train_test_split` from scikit-learn.

## Copy Files

- `copy_files` is a custom function that copies a specified number of files from the source directory to the destination directory. It selects files randomly.
- The function calculates the number of files to copy based on the desired number of files per class, ensuring it doesn't exceed the number of available files.
- For each class folder, files are copied to the corresponding subdirectories in the training, validation, and test sets, ensuring that the subdirectories exist.

## Organize Data

- The code organizes the data by creating subdirectories for each class in the training, validation, and test sets.
- It ensures that the necessary subdirectories are created for each class.
- Files are then copied from the original class directories to the appropriate subdirectories in the train, validation, and test sets.

## Summary

- Finally, the code prints the number of files in each set, providing an overview of how many images are available for each class within the training, validation, and test sets.

  ## Display Random Images from a Class

- `train_dir`: Specifies the path to the training dataset directory, which is "/kaggle/working/dataset/train".

- `class_folders`: Lists the subfolders (class folders) within the training directory by using `os.listdir(train_dir)`.

- `random_class`: Chooses a random class (subfolder) from the list of class folders using `random.choice(class_folders)`.

- `class_dir`: Generates the path to the directory of the randomly selected class by joining `train_dir` and `random_class`.

- `image_files`: Lists the image files within the selected class directory using `os.listdir(class_dir)`.

- `random_images`: Randomly selects five images from the list of image files using `random.sample(image_files, 5)`.

- Display Random Images: The code sets up a Matplotlib figure to display the selected images. It creates a row of subplots with a total of 5 subplots (one for each image).

  - For each selected image, it loads the image using `mpimg.imread(img_path)`.
  - It then adds the image to the corresponding subplot using `plt.imshow(img)`.
  - The image filename is set as the title of the subplot with `plt.title(image_file)`.
  - The axes (axis ticks and labels) are turned off using `plt.axis('off')`.

- Finally, it displays the figure containing the selected images with `plt.show()`.

This code snippet is used to visualize five random images from a randomly selected class within the training dataset, providing a visual representation of the data.


## Convert to TensorFlow Datasets

- `test_dir`, `train_dir`, and `validation_dir` are set to the directories containing the testing, training, and validation datasets, respectively, within the working directory.

- `batch_size` is defined as 32, specifying the number of images to process in each batch during training and evaluation.

- `image_size` is set to (180, 180), representing the desired size for the input images. You can adjust this size based on your specific requirements.

- The code loads the datasets using `tf.keras.utils.image_dataset_from_directory`, which is a convenient way to create TensorFlow datasets from image files within directories.

- `train_dataset` is created by loading images from the `train_dir`. The images are resized to the specified `image_size`, grouped into batches of `batch_size`, shuffled (with `shuffle=True`), and a seed of 42 is used for reproducibility.

- `validation_dataset` is similarly created from the `validation_dir`, but it's not shuffled since shuffling is generally not necessary for validation data.

- `test_dataset` is created from the `test_dir`, similar to the validation dataset. It is also not shuffled.

This section of the code converts the training, validation, and test datasets into TensorFlow datasets, making them suitable for use in training and evaluating machine learning models using TensorFlow.

## Build the Model

- `inputs`: Defines the input layer for the model with a shape of (180, 180, 3), which corresponds to images with dimensions 180x180 pixels and three color channels (RGB).

- `x = layers.Rescaling(1./255)(inputs)`: Rescales the input data to the [0, 1] range by dividing each pixel value by 255. This step is common preprocessing for image data to ensure it's in the appropriate range.

- The code defines a series of convolutional layers (`Conv2D`) and max-pooling layers (`MaxPool2D`) to create a Convolutional Neural Network (CNN) model.

  - `layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)`: The first convolutional layer with 32 filters, a 3x3 kernel size, and ReLU activation function.

  - `layers.MaxPool2D(pool_size=2)(x)`: The first max-pooling layer with a 2x2 pool size.

  - Several more convolutional and max-pooling layers are defined with increasing numbers of filters (64, 128, 256) and ReLU activations.

  - `layers.Flatten()`: Flattens the output from the convolutional layers into a 1D vector to prepare it for the dense layers.

- `outputs = layers.Dense(10, activation="softmax")(x)`: The output layer is a dense layer with 10 units (representing the 10 possible classes) and a softmax activation function, which is suitable for multi-class classification.

- `model = keras.Model(inputs=inputs, outputs=outputs)`: The code creates a model using the defined input and output layers, producing a complete neural network model.

This code snippet defines a CNN model architecture for image classification, with convolutional layers to extract features and a dense output layer for class prediction.

