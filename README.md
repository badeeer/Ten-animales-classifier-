 Ten-animales-classifier
 =================================

# Introduction: 

This  Python notebook provid a comprehensive image classification project using TensorFlow. It covers data organization, Convolutional Neural Network (CNN) model construction, dataset loading, model training with callbacks, feature extraction from a pre-trained VGG16 model, fine-tuning, and evaluation of the fine-tuned model's performance on a test dataset. This script demonstrates a holistic workflow for image classification.


## Import necessary libraries and modules

- `os`: This module allows interaction with the operating system, enabling file and directory operations.
- `shutil`: The `shutil` module is used for high-level file operations, such as copying and moving files and directories.
- `train_test_split` from `sklearn.model_selection`: This function is essential for splitting datasets into training and testing sets, a common task in machine learning.
- `random`: The `random` module provides functions for generating random numbers and performing randomization, which can be useful for various tasks, including data shuffling.
- `matplotlib.pyplot as plt`: `matplotlib` is a popular library for creating data visualizations. `pyplot` is a module within `matplotlib` that provides a simple and interactive way to create plots and charts.
- `matplotlib.image as mpimg`: The `mpimg` module is used for working with images, including loading and displaying them.
- `tensorflow as tf`: TensorFlow is a deep learning framework, and it is imported as `tf`. It is used for building and training machine learning models.
- `keras` from `tensorflow`: Keras is a high-level neural networks API that runs on top of TensorFlow. It is widely used for defining and training deep learning models.
- `layers, regularizers` from `tensorflow.keras`: These are submodules of the Keras library and provide tools for building and configuring neural network layers and applying regularization techniques to models.
- `numpy as np`: NumPy is a fundamental library for numerical operations in Python. It provides support for arrays, matrices, and various mathematical operations, which are essential for data processing in machine learning.

These imports set the stage for various operations related to data manipulation, model construction, and visualization in the subsequent code.

## Data Directory Setup

- `input_data_dir`: Specifies the directory where the original read-only dataset is located, in this case, "/kaggle/input/animals10/raw-img".
- `output_dir`: Defines the directory where the working dataset will be created, which is "/kaggle/working/dataset".

## Create Subdirectories

- `train_dir`, `validation_dir`, and `test_dir` are defined as subdirectories within the `output_dir`. These will be used to store the training, validation, and test data, respectively.
- `os.makedirs` is used to create these subdirectories if they don't already exist, setting `exist_ok=True`.

## Split Data into Sets

- `class_folders` is a list of class folders (e.g., categories of animals) within the input data directory.
- `train_classes`, `validation_classes`, and `test_classes` are generated by splitting the class folders into training, validation, and testing sets using `train_test_split` from scikit-learn.

## Copy Files

- `copy_files` is a custom function that copies a specified number of files from the source directory to the destination directory. It selects files randomly.
- The function calculates the number of files to copy based on the desired number of files per class, ensuring it doesn't exceed the number of available files.
- For each class folder, files are copied to the corresponding subdirectories in the training, validation, and test sets, ensuring that the subdirectories exist.

## Organize Data

- The code organizes the data by creating subdirectories for each class in the training, validation, and test sets.
- It ensures that the necessary subdirectories are created for each class.
- Files are then copied from the original class directories to the appropriate subdirectories in the train, validation, and test sets.

## Summary

- Finally, the code prints the number of files in each set, providing an overview of how many images are available for each class within the training, validation, and test sets.

  ## Display Random Images from a Class

- `train_dir`: Specifies the path to the training dataset directory, which is "/kaggle/working/dataset/train".

- `class_folders`: Lists the subfolders (class folders) within the training directory by using `os.listdir(train_dir)`.

- `random_class`: Chooses a random class (subfolder) from the list of class folders using `random.choice(class_folders)`.

- `class_dir`: Generates the path to the directory of the randomly selected class by joining `train_dir` and `random_class`.

- `image_files`: Lists the image files within the selected class directory using `os.listdir(class_dir)`.

- `random_images`: Randomly selects five images from the list of image files using `random.sample(image_files, 5)`.

- Display Random Images: The code sets up a Matplotlib figure to display the selected images. It creates a row of subplots with a total of 5 subplots (one for each image).

  - For each selected image, it loads the image using `mpimg.imread(img_path)`.
  - It then adds the image to the corresponding subplot using `plt.imshow(img)`.
  - The image filename is set as the title of the subplot with `plt.title(image_file)`.
  - The axes (axis ticks and labels) are turned off using `plt.axis('off')`.

- Finally, it displays the figure containing the selected images with `plt.show()`.

This code snippet is used to visualize five random images from a randomly selected class within the training dataset, providing a visual representation of the data.


## Convert to TensorFlow Datasets

- `test_dir`, `train_dir`, and `validation_dir` are set to the directories containing the testing, training, and validation datasets, respectively, within the working directory.

- `batch_size` is defined as 32, specifying the number of images to process in each batch during training and evaluation.

- `image_size` is set to (180, 180), representing the desired size for the input images. You can adjust this size based on your specific requirements.

- The code loads the datasets using `tf.keras.utils.image_dataset_from_directory`, which is a convenient way to create TensorFlow datasets from image files within directories.

- `train_dataset` is created by loading images from the `train_dir`. The images are resized to the specified `image_size`, grouped into batches of `batch_size`, shuffled (with `shuffle=True`), and a seed of 42 is used for reproducibility.

- `validation_dataset` is similarly created from the `validation_dir`, but it's not shuffled since shuffling is generally not necessary for validation data.

- `test_dataset` is created from the `test_dir`, similar to the validation dataset. It is also not shuffled.

This section of the code converts the training, validation, and test datasets into TensorFlow datasets, making them suitable for use in training and evaluating machine learning models using TensorFlow.

## Build the Model

- `inputs`: Defines the input layer for the model with a shape of (180, 180, 3), which corresponds to images with dimensions 180x180 pixels and three color channels (RGB).

- `x = layers.Rescaling(1./255)(inputs)`: Rescales the input data to the [0, 1] range by dividing each pixel value by 255. This step is common preprocessing for image data to ensure it's in the appropriate range.

- The code defines a series of convolutional layers (`Conv2D`) and max-pooling layers (`MaxPool2D`) to create a Convolutional Neural Network (CNN) model.

  - `layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)`: The first convolutional layer with 32 filters, a 3x3 kernel size, and ReLU activation function.

  - `layers.MaxPool2D(pool_size=2)(x)`: The first max-pooling layer with a 2x2 pool size.

  - Several more convolutional and max-pooling layers are defined with increasing numbers of filters (64, 128, 256) and ReLU activations.

  - `layers.Flatten()`: Flattens the output from the convolutional layers into a 1D vector to prepare it for the dense layers.

- `outputs = layers.Dense(10, activation="softmax")(x)`: The output layer is a dense layer with 10 units (representing the 10 possible classes) and a softmax activation function, which is suitable for multi-class classification.

- `model = keras.Model(inputs=inputs, outputs=outputs)`: The code creates a model using the defined input and output layers, producing a complete neural network model.



- `model.summary()`: This code snippet prints a summary of the neural network model's architecture and parameters.

- The summary provides details for each layer in the model, including the layer's name, type, output shape, and the number of parameters.

- It helps you understand the structure of the model, visualize the flow of data through the layers, and assess the complexity of the model.

- This information is crucial for model evaluation and debugging, as it allows you to check if the model is constructed as intended and verify the number of trainable parameters.

## Plot Model Architecture

- `keras.utils.plot_model(model, "animals_model.png", show_shapes=True)`: This code generates a graphical representation of the model's architecture and saves it as an image file named "animals_model.png."

- `model`: The model whose architecture you want to visualize.

- `"animals_model.png"`: The name of the output image file where the model's architecture diagram will be saved.

- `show_shapes=True`: This parameter specifies that the shapes of the layers will be displayed in the diagram, which can be useful for understanding the dimensions of data as it flows through the layers.

- The generated diagram typically shows the structure of the model, including the input and output layers, as well as the connections between layers. It provides a visual representation of how data flows through the neural network.

- This visualization is helpful for understanding the model's complexity, identifying any issues in the architecture, and communicating the model's structure to others.

## Model Configuration for Training

- `model.compile()`: This code is used to configure the model for training by specifying important settings, including the loss function, optimizer, and evaluation metrics.

- `loss='sparse_categorical_crossentropy'`: The loss function is set to "sparse_categorical_crossentropy." This loss is commonly used for multi-class classification tasks when the target labels are integers. It measures the dissimilarity between the predicted class probabilities and the true labels.

- `optimizer=keras.optimizers.RMSprop(learning_rate=1e-5)`: The optimizer is set to RMSprop with a specified learning rate of 1e-5. The optimizer is responsible for updating the model's weights during training to minimize the loss function.

- `metrics=["accuracy"]`: The evaluation metric used during training is accuracy. This metric measures how well the model is performing by calculating the proportion of correctly predicted class labels.

- Configuring the model in this way is a crucial step before training. It defines the objective and methodology for updating the model's parameters to learn from the training data.

## Callbacks for Model Training

- Callbacks in machine learning are functions that can be applied at different stages of the training process to customize and control the training procedure. They are commonly used for tasks such as early stopping and model checkpointing.

- `keras.callbacks.EarlyStopping` is used to define an early stopping callback. It monitors the validation loss and stops training if the validation loss does not improve for a certain number of epochs.

  - `monitor='val_loss'`: The callback monitors the validation loss to decide when to stop training.

  - `patience=10`: It waits for 10 epochs with no improvement in the validation loss before stopping the training.

  - `restore_best_weights=True`: When training stops, it restores the model's weights to the best weights achieved during training. This helps prevent overfitting.

- `keras.callbacks.ModelCheckpoint` is used to define a model checkpoint callback. It saves the model's weights during training at specified checkpoints.

  - `filepath="convnet_from_scratch.keras"`: The path where the model's weights will be saved. In this case, they will be saved to a file named "convnet_from_scratch.keras."

  - `save_best_only=True`: It saves only the best model weights based on the validation loss. This ensures that you have the best-performing model saved.

  - `monitor="val_loss"`: The validation loss is used as the criterion for determining the best model.

- `callbacks = [early_stop, model_checkpoint]`: Both the early stopping and model checkpoint callbacks are combined into a list of callbacks to be used during model training.

- `model.fit()`: This method is used to train the model with the provided training dataset. The training process will run for 50 epochs and will use the validation dataset for monitoring and applying the defined callbacks.

- The `history` variable stores information about the training process, such as the training and validation loss and accuracy, and can be used for visualization and analysis.

These callbacks help to improve the training process by preventing overfitting (early stopping) and ensuring the best model weights are saved for future use (model checkpoint).

## Leveraging a Pretrained Model

- Leveraging a pretrained model is a common technique in deep learning, where you use a pre-trained neural network as a feature extractor or starting point for your specific task. In this case, the code is using the VGG16 model, which has been pretrained on a large dataset.

- `pre_model = keras.applications.vgg16.VGG16(
    weights="imagenet",
    include_top=False,
    input_shape=(180, 180, 3)
)`: This code creates a VGG16 model that has been pretrained on the "ImageNet" dataset.

  - `weights="imagenet"`: Specifies that the model should be initialized with the pre-trained weights from the ImageNet dataset. These weights have been learned from a massive collection of images and can capture a wide range of visual features.

  - `include_top=False`: This option excludes the final classification layer (top layer) from the VGG16 model. This is common when you want to use the model as a feature extractor or fine-tune it for a specific task.

  - `input_shape=(180, 180, 3)`: Specifies the shape of the input data expected by the VGG16 model. In this case, it's set to (180, 180, 3), which matches the input size used in the previous part of the code.

- By using a pretrained model like VGG16, you can benefit from the knowledge and features learned from a vast number of images, which can be especially helpful when you have limited data for your specific task.

- After creating `pre_model`, you can use it to extract features from your data or fine-tune it for your classification task.

  ## Extracting Features Using a Pretrained Model

- In this code snippet, features are extracted from the datasets (train, validation, and test) using a pretrained VGG16 model.

- `get_features_and_labels(dataset)`: This is a custom function that takes a dataset as input and returns extracted features and corresponding labels.

- `all_features` and `all_labels` are empty lists to store the extracted features and labels.

- A loop iterates over each batch of images and labels in the dataset:

  - `for images, labels in dataset:`

- For each batch of images, the code preprocesses the images using `keras.applications.vgg16.preprocess_input(images)`. This preprocessing step ensures that the input data is formatted in a way that is compatible with the VGG16 model.

- The preprocessed images are then passed through the pretrained VGG16 model using `features = pre_model.predict(preprocessed_images)`. This step extracts high-level features from the images.

- Extracted features and labels from each batch are appended to the `all_features` and `all_labels` lists.

- After processing all batches in the dataset, the code returns the concatenated features and labels using `np.concatenate(all_features)` and `np.concatenate(all_labels)`.

- The extracted features are then assigned to variables: `train_features`, `train_labels`, `val_features`, `val_labels`, and `test_features`, `test_labels`, for the training, validation, and test datasets, respectively.

- These extracted features can be used as input to a different model for classification or other tasks. By using features learned by the pretrained VGG16 model, you leverage the transfer learning concept to improve the performance of your model.

## Building a Custom Classifier on Extracted Features

- `train_features.shape`: This code snippet displays the shape of the extracted features from the training dataset. It provides information about the dimensions of the feature data.

- After displaying the shape of the extracted features, a custom classifier model is defined for your specific task.

- `inputs = keras.Input(shape=(5, 5, 512))`: Defines the input layer for the custom classifier. The specified shape (5, 5, 512) is based on the shape of the extracted features from the VGG16 model.

- `x = layers.Flatten()(inputs)`: Flattens the input data to transform it from a 3D shape (5, 5, 512) to a 1D vector.

- `x = layers.Dense(64)(x)`: Adds a dense (fully connected) layer with 64 units. This layer is responsible for learning patterns and relationships in the flattened feature data.

- `x = layers.Dropout(0.5)(x)`: Introduces dropout, a regularization technique, to prevent overfitting. A dropout rate of 0.5 means that 50% of the neurons in the previous layer will be dropped during each training step.

- `outputs = layers.Dense(10, activation="softmax")(x)`: Defines the output layer with 10 units, representing the possible classes in the classification task. The softmax activation function is used to compute class probabilities.

- `model = keras.Model(inputs, outputs)`: Constructs the custom classifier model by specifying the input and output layers.

- After defining the model architecture, the code configures the model for training:

  - `model.compile()`: This sets up the training configuration for the model.

  - `loss='sparse_categorical_crossentropy'`: The loss function is set to "sparse_categorical_crossentropy," which is suitable for multi-class classification tasks with integer labels.

  - `optimizer=keras.optimizers.RMSprop(learning_rate=1e-5)`: The RMSprop optimizer is used with a specific learning rate of 1e-5.

  - `metrics=["accuracy"]`: The model will monitor the accuracy metric during training to assess its performance.

- This code defines a custom classifier on top of the features extracted from the VGG16 model, allowing you to adapt the pretrained features for your specific classification task.

## Callbacks for Model Training with Feature Extraction

- In this code, callbacks are defined for training the custom classifier on top of the extracted features.

- `keras.callbacks.EarlyStopping` is used to define an early stopping callback, which can prevent overfitting and save time during training.

  - `monitor='val_loss'`: The callback monitors the validation loss to decide when to stop training.

  - `patience=10`: It waits for 10 epochs with no improvement in the validation loss before stopping the training.

  - `restore_best_weights=True`: When training stops, it restores the model's weights to the best weights achieved during training. This helps prevent overfitting.

- `keras.callbacks.ModelCheckpoint` is used to define a model checkpoint callback. It saves the model's weights during training at specified checkpoints.

  - `filepath="feature_extraction.keras"`: The path where the model's weights will be saved. In this case, they will be saved to a file named "feature_extraction.keras."

  - `save_best_only=True`: It saves only the best model weights based on the validation loss. This ensures that you have the best-performing model saved.

  - `monitor="val_loss"`: The validation loss is used as the criterion for determining the best model.

- `callbacks = [early_stop, model_checkpoint]`: Both the early stopping and model checkpoint callbacks are combined into a list of callbacks to be used during model training.

- `model.fit()`: This method is used to train the custom classifier model with the extracted features and labels. The training process will run for 50 epochs and will use the validation features and labels for monitoring and applying the defined callbacks.

- The `history` variable stores information about the training process, such as the training and validation loss and accuracy, and can be used for visualization and analysis.

- These callbacks help to improve the training process by preventing overfitting (early stopping) and ensuring the best model weights are saved for future use (model checkpoint).

## Fine-Tuning a Pretrained Model

- Fine-tuning is a technique in transfer learning where you adjust the weights of a pretrained model to better suit a specific task. In this code, the pretrained model (VGG16) is fine-tuned.

- `pre_model.trainable = True`: This code sets the `trainable` property of the pretrained VGG16 model to `True`, allowing the model's weights to be updated during training. This is the first step in enabling fine-tuning.

- `for layer in pre_model.layers[:-4]:`: A loop iterates through the layers of the pretrained VGG16 model, excluding the last 4 layers. This means that the weights of the last 4 layers will remain frozen, while the weights of the earlier layers can be updated.

- `layer.trainable = False`: Inside the loop, each layer's `trainable` property is set to `False`. This prevents the weights of these layers from being updated during training.

- By setting the last 4 layers as non-trainable, you effectively fine-tune the earlier layers to adapt to your specific classification task. This approach is often used to retain the general feature extraction capabilities of the pretrained model while tailoring it to a particular domain or dataset.

- After this configuration, you can further train the model to optimize its performance for the new task, which typically requires fewer epochs than training from scratch. Fine-tuning is a powerful technique for improving model performance on specific tasks.

## Training a Fine-Tuned Model with Callbacks

- This code snippet configures the fine-tuned custom classifier model for training and applies callbacks to monitor the training process.

- `model.compile()`: This method sets up the training configuration for the model. It defines:

  - `loss='sparse_categorical_crossentropy'`: The loss function is set to "sparse_categorical_crossentropy," which is suitable for multi-class classification tasks with integer labels.

  - `optimizer=keras.optimizers.RMSprop(learning_rate=1e-5)`: The RMSprop optimizer is used with a specific learning rate of 1e-5.

  - `metrics=["accuracy"]`: The model will monitor the accuracy metric during training to assess its performance.

- `keras.callbacks.EarlyStopping` and `keras.callbacks.ModelCheckpoint` are the same callbacks used as described in a previous explanation. They are defined here for the fine-tuning phase to prevent overfitting and save the best model weights.

- `model.fit()`: This method trains the fine-tuned model with the extracted features and labels. The training process will run for 50 epochs and will use the validation features and labels for monitoring and applying the defined callbacks.

- The `history` variable stores information about the training process, such as the training and validation loss and accuracy. It can be used for visualization and analysis.

- In this code, the model goes through the fine-tuning process with the adjusted layers, allowing it to optimize its performance for the specific classification task while retaining the general features learned from the pretrained model.

- Using callbacks like early stopping and model checkpointing is a good practice to ensure efficient and effective model training, especially when fine-tuning pretrained models.

## Evaluating the Model on Test Data

- In this code snippet, the fine-tuned model is evaluated on a separate test dataset to assess its performance.

- `test_loss, test_acc = model.evaluate(test_features, test_labels)`: This line evaluates the fine-tuned model using the test features and corresponding test labels. It computes the test loss and test accuracy.

- `test_loss` stores the test loss, and `test_acc` stores the test accuracy.

- `print(f"Test accuracy: {test_acc:.3f}")`: The code prints the test accuracy with three decimal places. This provides a quantitative measure of how well the model performs on unseen data.

- Evaluating the model on a separate test dataset is essential to determine its real-world performance. The test accuracy represents the model's ability to generalize to new, unseen data, which is a critical metric for assessing the quality of the model.

